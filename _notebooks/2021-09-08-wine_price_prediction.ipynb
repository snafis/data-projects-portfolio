```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import metrics 
from sklearn import linear_model
from sklearn.metrics import accuracy_score
```


```python
 df = pd.read_csv('wine2.csv')  
```


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>country</th>
      <th>description</th>
      <th>designation</th>
      <th>points</th>
      <th>price</th>
      <th>province</th>
      <th>region_1</th>
      <th>region_2</th>
      <th>taster_name</th>
      <th>taster_twitter_handle</th>
      <th>title</th>
      <th>variety</th>
      <th>winery</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>Italy</td>
      <td>Aromas include tropical fruit, broom, brimston...</td>
      <td>Vulkà Bianco</td>
      <td>87</td>
      <td>NaN</td>
      <td>Sicily &amp; Sardinia</td>
      <td>Etna</td>
      <td>NaN</td>
      <td>Kerin O’Keefe</td>
      <td>@kerinokeefe</td>
      <td>Nicosia 2013 Vulkà Bianco  (Etna)</td>
      <td>White Blend</td>
      <td>Nicosia</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>Portugal</td>
      <td>This is ripe and fruity, a wine that is smooth...</td>
      <td>Avidagos</td>
      <td>87</td>
      <td>15.0</td>
      <td>Douro</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Roger Voss</td>
      <td>@vossroger</td>
      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>
      <td>Portuguese Red</td>
      <td>Quinta dos Avidagos</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>US</td>
      <td>Tart and snappy, the flavors of lime flesh and...</td>
      <td>NaN</td>
      <td>87</td>
      <td>14.0</td>
      <td>Oregon</td>
      <td>Willamette Valley</td>
      <td>Willamette Valley</td>
      <td>Paul Gregutt</td>
      <td>@paulgwine</td>
      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>
      <td>Pinot Gris</td>
      <td>Rainstorm</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>US</td>
      <td>Pineapple rind, lemon pith and orange blossom ...</td>
      <td>Reserve Late Harvest</td>
      <td>87</td>
      <td>13.0</td>
      <td>Michigan</td>
      <td>Lake Michigan Shore</td>
      <td>NaN</td>
      <td>Alexander Peartree</td>
      <td>NaN</td>
      <td>St. Julian 2013 Reserve Late Harvest Riesling ...</td>
      <td>Riesling</td>
      <td>St. Julian</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>US</td>
      <td>Much like the regular bottling from 2012, this...</td>
      <td>Vintner's Reserve Wild Child Block</td>
      <td>87</td>
      <td>65.0</td>
      <td>Oregon</td>
      <td>Willamette Valley</td>
      <td>Willamette Valley</td>
      <td>Paul Gregutt</td>
      <td>@paulgwine</td>
      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child...</td>
      <td>Pinot Noir</td>
      <td>Sweet Cheeks</td>
    </tr>
  </tbody>
</table>
</div>



###### Cleaning Data


```python
df.drop(["Unnamed: 0"], axis=1, inplace=True) # got rid of unnamed column, no use for it
```


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>country</th>
      <th>description</th>
      <th>designation</th>
      <th>points</th>
      <th>price</th>
      <th>province</th>
      <th>region_1</th>
      <th>region_2</th>
      <th>taster_name</th>
      <th>taster_twitter_handle</th>
      <th>title</th>
      <th>variety</th>
      <th>winery</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Italy</td>
      <td>Aromas include tropical fruit, broom, brimston...</td>
      <td>Vulkà Bianco</td>
      <td>87</td>
      <td>NaN</td>
      <td>Sicily &amp; Sardinia</td>
      <td>Etna</td>
      <td>NaN</td>
      <td>Kerin O’Keefe</td>
      <td>@kerinokeefe</td>
      <td>Nicosia 2013 Vulkà Bianco  (Etna)</td>
      <td>White Blend</td>
      <td>Nicosia</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Portugal</td>
      <td>This is ripe and fruity, a wine that is smooth...</td>
      <td>Avidagos</td>
      <td>87</td>
      <td>15.0</td>
      <td>Douro</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Roger Voss</td>
      <td>@vossroger</td>
      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>
      <td>Portuguese Red</td>
      <td>Quinta dos Avidagos</td>
    </tr>
    <tr>
      <th>2</th>
      <td>US</td>
      <td>Tart and snappy, the flavors of lime flesh and...</td>
      <td>NaN</td>
      <td>87</td>
      <td>14.0</td>
      <td>Oregon</td>
      <td>Willamette Valley</td>
      <td>Willamette Valley</td>
      <td>Paul Gregutt</td>
      <td>@paulgwine</td>
      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>
      <td>Pinot Gris</td>
      <td>Rainstorm</td>
    </tr>
    <tr>
      <th>3</th>
      <td>US</td>
      <td>Pineapple rind, lemon pith and orange blossom ...</td>
      <td>Reserve Late Harvest</td>
      <td>87</td>
      <td>13.0</td>
      <td>Michigan</td>
      <td>Lake Michigan Shore</td>
      <td>NaN</td>
      <td>Alexander Peartree</td>
      <td>NaN</td>
      <td>St. Julian 2013 Reserve Late Harvest Riesling ...</td>
      <td>Riesling</td>
      <td>St. Julian</td>
    </tr>
    <tr>
      <th>4</th>
      <td>US</td>
      <td>Much like the regular bottling from 2012, this...</td>
      <td>Vintner's Reserve Wild Child Block</td>
      <td>87</td>
      <td>65.0</td>
      <td>Oregon</td>
      <td>Willamette Valley</td>
      <td>Willamette Valley</td>
      <td>Paul Gregutt</td>
      <td>@paulgwine</td>
      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child...</td>
      <td>Pinot Noir</td>
      <td>Sweet Cheeks</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_copy = df  # made a copy because we were making changes to the variables, and wanted to see the original data set still
```


```python
df.drop(["region_2"], axis=1, inplace=True)  # region_2 was dropped because it had too many NaN
```


```python
df.drop(['description','designation','taster_name','taster_twitter_handle','title'], axis=1, inplace=True)
# dropped the preceding variables as they either contained text, or unneccesary information
```


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>country</th>
      <th>points</th>
      <th>price</th>
      <th>province</th>
      <th>region_1</th>
      <th>variety</th>
      <th>winery</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Italy</td>
      <td>87</td>
      <td>NaN</td>
      <td>Sicily &amp; Sardinia</td>
      <td>Etna</td>
      <td>White Blend</td>
      <td>Nicosia</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Portugal</td>
      <td>87</td>
      <td>15.0</td>
      <td>Douro</td>
      <td>NaN</td>
      <td>Portuguese Red</td>
      <td>Quinta dos Avidagos</td>
    </tr>
    <tr>
      <th>2</th>
      <td>US</td>
      <td>87</td>
      <td>14.0</td>
      <td>Oregon</td>
      <td>Willamette Valley</td>
      <td>Pinot Gris</td>
      <td>Rainstorm</td>
    </tr>
    <tr>
      <th>3</th>
      <td>US</td>
      <td>87</td>
      <td>13.0</td>
      <td>Michigan</td>
      <td>Lake Michigan Shore</td>
      <td>Riesling</td>
      <td>St. Julian</td>
    </tr>
    <tr>
      <th>4</th>
      <td>US</td>
      <td>87</td>
      <td>65.0</td>
      <td>Oregon</td>
      <td>Willamette Valley</td>
      <td>Pinot Noir</td>
      <td>Sweet Cheeks</td>
    </tr>
  </tbody>
</table>
</div>




```python
df = df[['price','country','points','province','region_1','variety','winery']] #changed the order for ease of use
```


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>country</th>
      <th>points</th>
      <th>province</th>
      <th>region_1</th>
      <th>variety</th>
      <th>winery</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>NaN</td>
      <td>Italy</td>
      <td>87</td>
      <td>Sicily &amp; Sardinia</td>
      <td>Etna</td>
      <td>White Blend</td>
      <td>Nicosia</td>
    </tr>
    <tr>
      <th>1</th>
      <td>15.0</td>
      <td>Portugal</td>
      <td>87</td>
      <td>Douro</td>
      <td>NaN</td>
      <td>Portuguese Red</td>
      <td>Quinta dos Avidagos</td>
    </tr>
    <tr>
      <th>2</th>
      <td>14.0</td>
      <td>US</td>
      <td>87</td>
      <td>Oregon</td>
      <td>Willamette Valley</td>
      <td>Pinot Gris</td>
      <td>Rainstorm</td>
    </tr>
    <tr>
      <th>3</th>
      <td>13.0</td>
      <td>US</td>
      <td>87</td>
      <td>Michigan</td>
      <td>Lake Michigan Shore</td>
      <td>Riesling</td>
      <td>St. Julian</td>
    </tr>
    <tr>
      <th>4</th>
      <td>65.0</td>
      <td>US</td>
      <td>87</td>
      <td>Oregon</td>
      <td>Willamette Valley</td>
      <td>Pinot Noir</td>
      <td>Sweet Cheeks</td>
    </tr>
  </tbody>
</table>
</div>




```python
from sklearn import preprocessing 
LE = preprocessing.LabelEncoder()            #taking string of letters and numbers and coding into #s
df["country"] = LE.fit_transform(df["country"].astype(str))
df["province"] = LE.fit_transform(df["province"].astype(str))   
df["region_1"] = LE.fit_transform(df["region_1"].astype(str))  
df["variety"] = LE.fit_transform(df["variety"].astype(str))
df["winery"] = LE.fit_transform(df["winery"].astype(str))
```


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>country</th>
      <th>points</th>
      <th>province</th>
      <th>region_1</th>
      <th>variety</th>
      <th>winery</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>NaN</td>
      <td>22</td>
      <td>87</td>
      <td>331</td>
      <td>424</td>
      <td>690</td>
      <td>11608</td>
    </tr>
    <tr>
      <th>1</th>
      <td>15.0</td>
      <td>31</td>
      <td>87</td>
      <td>108</td>
      <td>1229</td>
      <td>450</td>
      <td>12956</td>
    </tr>
    <tr>
      <th>2</th>
      <td>14.0</td>
      <td>40</td>
      <td>87</td>
      <td>268</td>
      <td>1217</td>
      <td>436</td>
      <td>13018</td>
    </tr>
    <tr>
      <th>3</th>
      <td>13.0</td>
      <td>40</td>
      <td>87</td>
      <td>218</td>
      <td>549</td>
      <td>479</td>
      <td>14390</td>
    </tr>
    <tr>
      <th>4</th>
      <td>65.0</td>
      <td>40</td>
      <td>87</td>
      <td>268</td>
      <td>1217</td>
      <td>440</td>
      <td>14621</td>
    </tr>
  </tbody>
</table>
</div>




```python
df = df.dropna()   #got rid of NaN
```


```python
X = df.values[:,1:len(df)]
Y = df.values[:,0]       # defined x and y 
```


```python
X_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size = 0.3, random_state = 100) 
# split data, 30% test, 70% train
```

##### Testing Linear Model (due to continuous variable prediction)


```python
LR = linear_model.LinearRegression()
```


```python
LR.fit(X_train, Y_train)
```




    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,
             normalize=False)




```python
LR.score(X_train, Y_train)
```




    0.1759460258431388




```python
LR.coef_   
```




    array([ 1.30826216e-02,  5.60808418e+00, -1.14806927e-02, -3.70507725e-03,
           -1.21449835e-02, -2.81913833e-05])




```python
y_LR = LR.predict(X_test)
```


```python
mae=metrics.mean_absolute_error(Y_test,y_LR)
mse=metrics.mean_squared_error(Y_test,y_LR)
rmse=np.sqrt(mse)
```


```python
print(mae,mse,rmse)  #how far away is our prediction from the actual price, approx $16 off, 
# this seems poor but our prices range from $4 to $3300
```

    16.888294995274837 1250.940567456863 35.36863819059002


##### Random Forest


```python
from sklearn.ensemble import RandomForestClassifier
```


```python
rf = RandomForestClassifier()
# next trying random forest
```


```python
X_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size = 0.3, random_state = 100)
Y_train=np.asarray(Y_train,dtype=np.float64) # had to change the price to float because TypeError occurred
Y_test = np.asarray(Y_test,dtype=np.float64)
type(Y_train)
```




    numpy.ndarray




```python
rf.fit(X_train, Y_train) 
rf.score(X_train, Y_train)
y_rf = rf.predict(X_test)
```

    /anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
      "10 in version 0.20 to 100 in 0.22.", FutureWarning)



```python
rf.score(X_train, Y_train) #had to check for score as opposed to accuracy score because it is not a classifier
```




    0.8724168064051392




```python
# Using error measures to check strength of prediction
mae=metrics.mean_absolute_error(Y_test,y_rf)
mse=metrics.mean_squared_error(Y_test,y_rf)
rmse=np.sqrt(mse)
```


```python
print(mae,mse,rmse)   #random forest better measure so far, as we see lower error, i.e. from $16 in error to $12
# BUT, when comparing rmse, linear model is lower (better)
```

    12.361915520899347 993.6114677761552 31.52160319171846


##### Decision Tree


```python
from sklearn.tree import DecisionTreeClassifier
```


```python
clf_gini = DecisionTreeClassifier(criterion = "gini", random_state = 100, max_depth = 3, min_samples_leaf = 5)
```


```python
clf_gini.fit(X_train, Y_train)
```




    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,
                max_features=None, max_leaf_nodes=None,
                min_impurity_decrease=0.0, min_impurity_split=None,
                min_samples_leaf=5, min_samples_split=2,
                min_weight_fraction_leaf=0.0, presort=False, random_state=100,
                splitter='best')




```python
clf_entropy = DecisionTreeClassifier(criterion = "entropy", random_state = 100, max_depth = 3, min_samples_leaf=5)
clf_entropy.fit(X_train, Y_train)
```




    DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,
                max_features=None, max_leaf_nodes=None,
                min_impurity_decrease=0.0, min_impurity_split=None,
                min_samples_leaf=5, min_samples_split=2,
                min_weight_fraction_leaf=0.0, presort=False, random_state=100,
                splitter='best')




```python
y_pred_g = clf_gini.predict(X_test)  #prediction of y using gini model, always use x test for pred
y_pred_g
```




    array([15., 20., 15., ..., 10., 50., 20.])




```python
y_pred_e = clf_entropy.predict(X_test)  #if prob is less than 50% it rounds to zero, and if its over it rounds to 1
y_pred_e
```




    array([20., 25., 20., ..., 25., 50., 20.])




```python
clf_gini.score(X_train, Y_train)
```




    0.07594293946765547




```python
clf_entropy.score(X_train, Y_train) # compared gini vs. entropy methods
```




    0.07309699818143171




```python
mae=metrics.mean_absolute_error(Y_test,y_pred_g)
mse=metrics.mean_squared_error(Y_test,y_pred_g)
rmse=np.sqrt(mse)
```


```python
print(mae,mse,rmse)
```

    15.688810514424269 1421.6534868983 37.704820472962076



```python
mae=metrics.mean_absolute_error(Y_test,y_pred_e)
mse=metrics.mean_squared_error(Y_test,y_pred_e)
rmse=np.sqrt(mse)
```


```python
print(mae,mse,rmse)
```

    15.748822086903811 1415.5991513515003 37.62444885113269


##### K Nearest Neighbors


```python
from sklearn.neighbors import KNeighborsClassifier  
```


```python
K_value = list(range(1,25))
for k in K_value:
    neigh = KNeighborsClassifier(n_neighbors = k, weights='uniform', algorithm='auto')  
    neigh.fit(X_train, Y_train)  
    y_pred = neigh.predict(X_test)
    print("K =",k)
    print("Mean Absolute Error:", metrics.mean_absolute_error(Y_test,y_pred))
    print("Mean Squared Error:", metrics.mean_squared_error(Y_test,y_pred)) 
    print("Root Mean Squared Error:", np.sqrt(metrics.mean_squared_error(Y_test,y_pred)))  
    # Testing k nearest neighbors algorithm, looping through k=1 to k=25
    # Using error measures to evaluate model strength
    # 
```

    K = 1
    Mean Absolute Error: 11.72531893202546
    Mean Squared Error: 1211.3479458848815
    Root Mean Squared Error: 34.80442422860751
    K = 2
    Mean Absolute Error: 11.902956492987629
    Mean Squared Error: 1050.7298928167966
    Root Mean Squared Error: 32.41496402615305
    K = 3
    Mean Absolute Error: 13.16281376573995
    Mean Squared Error: 1210.6322431322844
    Root Mean Squared Error: 34.79414093108615
    K = 4
    Mean Absolute Error: 14.084892403493786
    Mean Squared Error: 1264.8619568511833
    Root Mean Squared Error: 35.56489781865236
    K = 5
    Mean Absolute Error: 14.697214338853222
    Mean Squared Error: 1340.0268922381726
    Root Mean Squared Error: 36.606377753585136
    K = 6
    Mean Absolute Error: 15.120546661890723
    Mean Squared Error: 1385.9752569365994
    Root Mean Squared Error: 37.228688627677975
    K = 7
    Mean Absolute Error: 15.385942192709338
    Mean Squared Error: 1410.9534069930842
    Root Mean Squared Error: 37.562659743328666
    K = 8
    Mean Absolute Error: 15.59121593695754
    Mean Squared Error: 1426.422395503265
    Root Mean Squared Error: 37.768007565971295
    K = 9
    Mean Absolute Error: 15.687598159424683
    Mean Squared Error: 1416.9882346457994
    Root Mean Squared Error: 37.642904173905066
    K = 10
    Mean Absolute Error: 15.850219050505608
    Mean Squared Error: 1435.1590113796049
    Root Mean Squared Error: 37.88349259743094
    K = 11
    Mean Absolute Error: 16.012288871132174
    Mean Squared Error: 1486.143664067451
    Root Mean Squared Error: 38.55053390119845
    K = 12
    Mean Absolute Error: 16.25726724161684
    Mean Squared Error: 1516.4430330917808
    Root Mean Squared Error: 38.94153352260002
    K = 13
    Mean Absolute Error: 16.40343867963519
    Mean Squared Error: 1525.9303998016146
    Root Mean Squared Error: 39.063159111899985
    K = 14
    Mean Absolute Error: 16.577273854462295
    Mean Squared Error: 1535.2365194390102
    Root Mean Squared Error: 39.18209437280006
    K = 15
    Mean Absolute Error: 16.783071115642134
    Mean Squared Error: 1555.46948447359
    Root Mean Squared Error: 39.439440722119656
    K = 16
    Mean Absolute Error: 16.906621111509107
    Mean Squared Error: 1563.0005235169317
    Root Mean Squared Error: 39.53480142250536
    K = 17
    Mean Absolute Error: 16.99335960102499
    Mean Squared Error: 1572.6054060011572
    Root Mean Squared Error: 39.65608914153231
    K = 18
    Mean Absolute Error: 17.110902928939463
    Mean Squared Error: 1585.0617474444107
    Root Mean Squared Error: 39.81283395394519
    K = 19
    Mean Absolute Error: 17.202656159590003
    Mean Squared Error: 1591.9101755159397
    Root Mean Squared Error: 39.898749047005715
    K = 20
    Mean Absolute Error: 17.303336731601135
    Mean Squared Error: 1592.4133579478137
    Root Mean Squared Error: 39.90505429075136
    K = 21
    Mean Absolute Error: 17.309729148871682
    Mean Squared Error: 1585.4476620835974
    Root Mean Squared Error: 39.817680269995606
    K = 22
    Mean Absolute Error: 17.368776348056098
    Mean Squared Error: 1591.4024467528172
    Root Mean Squared Error: 39.892385824275
    K = 23
    Mean Absolute Error: 17.48309591381258
    Mean Squared Error: 1604.7787727660982
    Root Mean Squared Error: 40.059690123191146
    K = 24
    Mean Absolute Error: 17.52263521891274
    Mean Squared Error: 1601.0931860138319
    Root Mean Squared Error: 40.01366249187684


#### Descriptive Stats


```python
mean_price = sum(df['price'])/len(df['price'])
mean_price   # to see mean price of wine
```




    35.363389129985535




```python
import matplotlib.pyplot as plt
%matplotlib inline
plt.boxplot(df['price'])  # wanted to see range of price points to look for outliers
```




    {'whiskers': [<matplotlib.lines.Line2D at 0x1a21ed8828>,
      <matplotlib.lines.Line2D at 0x1a21ed8b70>],
     'caps': [<matplotlib.lines.Line2D at 0x1a21ed8eb8>,
      <matplotlib.lines.Line2D at 0x1a21ed8f98>],
     'boxes': [<matplotlib.lines.Line2D at 0x1a21ed8400>],
     'medians': [<matplotlib.lines.Line2D at 0x1a223ed588>],
     'fliers': [<matplotlib.lines.Line2D at 0x1a223ed8d0>],
     'means': []}




    
![png](output_51_1.png)
    



```python
y_rf
```




    array([25., 11., 40., ..., 40., 85., 16.])




```python
print(y_rf)
```

    [15. 11. 40. ... 40. 85. 12.]



```python
print(y_rf,Y_test)
```

    [15. 11. 40. ... 40. 85. 12.] [25. 11. 35. ... 40. 40. 15.]


### Is there a relationship between price and points?


```python
import matplotlib
%matplotlib inline
matplotlib.style.use('ggplot')

plt.scatter(df['price'], df['points'])
plt.show() # Plot price vs points to look for correlation
```


    
![png](output_56_0.png)
    



```python
np.corrcoef(df['price'], df['points'])[0, 1] # Correlation did not look strong, so checked correlation 
# coefficient to determine if there is a relationship between price and points
```




    0.4161667418606225



##### Changing our prediction from continuous to categorical


```python
df2 = df
```


```python
df2['price'] = pd.qcut(df2['price'], q = 10) # Cut data into deciles (so each price category contains ~10% of data)
```


```python
df2.head() # Round bracket = not inclusive of, square bracket means inclusive of.  
# So range (12.0,15.0] does not include 12.00 but would include 12.01 up to and including 15.00
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>country</th>
      <th>points</th>
      <th>province</th>
      <th>region_1</th>
      <th>variety</th>
      <th>winery</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>(12.0, 15.0]</td>
      <td>31</td>
      <td>87</td>
      <td>108</td>
      <td>1229</td>
      <td>450</td>
      <td>12956</td>
    </tr>
    <tr>
      <th>2</th>
      <td>(12.0, 15.0]</td>
      <td>40</td>
      <td>87</td>
      <td>268</td>
      <td>1217</td>
      <td>436</td>
      <td>13018</td>
    </tr>
    <tr>
      <th>3</th>
      <td>(12.0, 15.0]</td>
      <td>40</td>
      <td>87</td>
      <td>218</td>
      <td>549</td>
      <td>479</td>
      <td>14390</td>
    </tr>
    <tr>
      <th>4</th>
      <td>(48.0, 65.0]</td>
      <td>40</td>
      <td>87</td>
      <td>268</td>
      <td>1217</td>
      <td>440</td>
      <td>14621</td>
    </tr>
    <tr>
      <th>5</th>
      <td>(12.0, 15.0]</td>
      <td>37</td>
      <td>87</td>
      <td>262</td>
      <td>757</td>
      <td>590</td>
      <td>14706</td>
    </tr>
  </tbody>
</table>
</div>




```python
df2['price'] = LE.fit_transform(df2['price'].astype(str)) # Need to transform to numeric from string
```


```python
LE.inverse_transform([0,1,2,3,4,5,6,7,8,9])  # checking what the encoded labels are for future reference
# for example, 0 represents the range of (12.0, 15.0]
```




    array(['(12.0, 15.0]', '(15.0, 18.0]', '(18.0, 22.0]', '(22.0, 25.0]',
           '(25.0, 30.0]', '(3.999, 12.0]', '(30.0, 38.0]', '(38.0, 48.0]',
           '(48.0, 65.0]', '(65.0, 3300.0]'], dtype=object)




```python
df2.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>country</th>
      <th>points</th>
      <th>province</th>
      <th>region_1</th>
      <th>variety</th>
      <th>winery</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>31</td>
      <td>87</td>
      <td>108</td>
      <td>1229</td>
      <td>450</td>
      <td>12956</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>40</td>
      <td>87</td>
      <td>268</td>
      <td>1217</td>
      <td>436</td>
      <td>13018</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>40</td>
      <td>87</td>
      <td>218</td>
      <td>549</td>
      <td>479</td>
      <td>14390</td>
    </tr>
    <tr>
      <th>4</th>
      <td>8</td>
      <td>40</td>
      <td>87</td>
      <td>268</td>
      <td>1217</td>
      <td>440</td>
      <td>14621</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>37</td>
      <td>87</td>
      <td>262</td>
      <td>757</td>
      <td>590</td>
      <td>14706</td>
    </tr>
  </tbody>
</table>
</div>




```python
X2 = df.values[:,1:len(df2)]
Y2 = df.values[:,0]
```


```python
print(Y2)
```

    [0 0 0 ... 4 6 2]



```python
X_train2, X_test2, Y_train2, Y_test2 = train_test_split (X2, Y2, test_size = 0.3, random_state = 100)
```

#### Multinomial Logisitic Regression (now we are doing categorical, and have more than two categories)


```python
m_log = linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs')
m_log.fit(X_train2, Y_train2)
```

    /anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.
      "of iterations.", ConvergenceWarning)





    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
              intercept_scaling=1, max_iter=100, multi_class='multinomial',
              n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',
              tol=0.0001, verbose=0, warm_start=False)




```python
print(m_log.coef_)
```

    [[-1.00249460e-03 -7.72870135e-04  5.55106631e-04  1.40924131e-04
       3.12396359e-05 -4.53691276e-06]
     [-5.70768955e-04 -7.41361763e-04  8.90186824e-04 -2.34941320e-05
      -1.24693613e-04 -7.34274245e-06]
     [-4.10618136e-04 -1.32415663e-04  6.14252071e-04  4.78843548e-05
      -5.91375946e-05  5.17927289e-06]
     [-5.43988192e-05 -6.18609023e-04 -5.98673398e-05 -1.07518177e-04
       4.44589145e-05 -6.01130181e-07]
     [ 4.73289147e-04 -2.21327306e-04  1.52216766e-04  9.03175642e-05
       2.66740477e-05 -6.84577959e-06]
     [-1.23241269e-03 -1.48981899e-03  5.85477804e-06  2.56407820e-04
       2.05871937e-04 -7.56629235e-06]
     [ 7.70172308e-04 -1.52146862e-04 -1.12679329e-03  7.02689777e-05
       3.13237056e-04 -6.57120114e-07]
     [ 9.64383993e-04  6.27632472e-04 -8.83641803e-04  6.51205435e-05
       1.85953784e-04  4.26032747e-07]
     [ 8.83141219e-04  1.31220644e-03 -6.08160161e-04 -1.23139138e-04
      -2.21805687e-05  1.17844414e-05]
     [ 1.79706539e-04  2.18871083e-03  4.60845521e-04 -4.16771943e-04
      -6.01423598e-04  1.01602304e-05]]



```python
pred_mlog = m_log.predict(X_test2)
```


```python
metrics.accuracy_score(Y_test2, pred_mlog)
```




    0.13881464745267683




```python
LE.inverse_transform([0,1,2,3,4,5,6,7,8,9])  # checking what the encoded labels are for confusion matrix
```




    array(['(12.0, 15.0]', '(15.0, 18.0]', '(18.0, 22.0]', '(22.0, 25.0]',
           '(25.0, 30.0]', '(3.999, 12.0]', '(30.0, 38.0]', '(38.0, 48.0]',
           '(48.0, 65.0]', '(65.0, 3300.0]'], dtype=object)




```python
pd.DataFrame(metrics.confusion_matrix(Y_test2, pred_mlog)) # converted confusion matrix from array to 
# data frame for easier viewing
# x-axis is predicted value, and y-axis is actual value
# this shows is the model is not strong, as it was correct in predicting the '0' range score 0 times
# on average, it results in the accuracy score of ~ 13%
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>259</td>
      <td>13</td>
      <td>1742</td>
      <td>0</td>
      <td>1</td>
      <td>325</td>
      <td>25</td>
      <td>583</td>
      <td>513</td>
      <td>301</td>
    </tr>
    <tr>
      <th>1</th>
      <td>207</td>
      <td>11</td>
      <td>1693</td>
      <td>0</td>
      <td>1</td>
      <td>213</td>
      <td>25</td>
      <td>645</td>
      <td>497</td>
      <td>223</td>
    </tr>
    <tr>
      <th>2</th>
      <td>283</td>
      <td>11</td>
      <td>1985</td>
      <td>0</td>
      <td>4</td>
      <td>261</td>
      <td>31</td>
      <td>938</td>
      <td>591</td>
      <td>220</td>
    </tr>
    <tr>
      <th>3</th>
      <td>160</td>
      <td>9</td>
      <td>1295</td>
      <td>0</td>
      <td>2</td>
      <td>146</td>
      <td>22</td>
      <td>824</td>
      <td>471</td>
      <td>143</td>
    </tr>
    <tr>
      <th>4</th>
      <td>171</td>
      <td>9</td>
      <td>1449</td>
      <td>0</td>
      <td>2</td>
      <td>168</td>
      <td>11</td>
      <td>1034</td>
      <td>595</td>
      <td>219</td>
    </tr>
    <tr>
      <th>5</th>
      <td>306</td>
      <td>11</td>
      <td>1647</td>
      <td>0</td>
      <td>0</td>
      <td>457</td>
      <td>15</td>
      <td>507</td>
      <td>514</td>
      <td>206</td>
    </tr>
    <tr>
      <th>6</th>
      <td>156</td>
      <td>19</td>
      <td>1148</td>
      <td>0</td>
      <td>0</td>
      <td>134</td>
      <td>26</td>
      <td>1113</td>
      <td>621</td>
      <td>214</td>
    </tr>
    <tr>
      <th>7</th>
      <td>123</td>
      <td>14</td>
      <td>1230</td>
      <td>0</td>
      <td>0</td>
      <td>139</td>
      <td>18</td>
      <td>1321</td>
      <td>618</td>
      <td>249</td>
    </tr>
    <tr>
      <th>8</th>
      <td>151</td>
      <td>11</td>
      <td>1378</td>
      <td>0</td>
      <td>0</td>
      <td>124</td>
      <td>4</td>
      <td>1191</td>
      <td>647</td>
      <td>304</td>
    </tr>
    <tr>
      <th>9</th>
      <td>145</td>
      <td>5</td>
      <td>1184</td>
      <td>0</td>
      <td>0</td>
      <td>101</td>
      <td>1</td>
      <td>861</td>
      <td>719</td>
      <td>330</td>
    </tr>
  </tbody>
</table>
</div>



#### Decision Tree with Categorical


```python
clf_gini = DecisionTreeClassifier(criterion = "gini", random_state = 100, max_depth = 3, min_samples_leaf = 5)
```


```python
clf_gini.fit(X_train2, Y_train2)
```




    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,
                max_features=None, max_leaf_nodes=None,
                min_impurity_decrease=0.0, min_impurity_split=None,
                min_samples_leaf=5, min_samples_split=2,
                min_weight_fraction_leaf=0.0, presort=False, random_state=100,
                splitter='best')




```python
clf_entropy = DecisionTreeClassifier(criterion = "entropy", random_state = 100, max_depth = 3, min_samples_leaf=5)
clf_entropy.fit(X_train2, Y_train2)
```




    DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,
                max_features=None, max_leaf_nodes=None,
                min_impurity_decrease=0.0, min_impurity_split=None,
                min_samples_leaf=5, min_samples_split=2,
                min_weight_fraction_leaf=0.0, presort=False, random_state=100,
                splitter='best')




```python
ypred_g_cat = clf_gini.predict(X_test2)  #prediction of y using gini model, always use x test for pred
ypred_g_cat
```




    array([2, 4, 2, ..., 4, 8, 5])




```python
ypred_e_cat = clf_entropy.predict(X_test2)  #prediction of y using gini model, always use x test for pred
ypred_e_cat
```




    array([2, 4, 2, ..., 4, 8, 5])




```python
metrics.accuracy_score(Y_test2, ypred_g_cat)
```




    0.22213650015154437




```python
metrics.accuracy_score(Y_test2, ypred_e_cat)
```




    0.22213650015154437




```python
pd.DataFrame(metrics.confusion_matrix(Y_test2, ypred_g_cat)) # Confusion matrix and accuracy score show that this 
# is not as accurate as the multi logistic regression
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>1147</td>
      <td>0</td>
      <td>450</td>
      <td>1873</td>
      <td>220</td>
      <td>0</td>
      <td>67</td>
      <td>5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>1094</td>
      <td>0</td>
      <td>600</td>
      <td>1266</td>
      <td>363</td>
      <td>0</td>
      <td>174</td>
      <td>18</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>1381</td>
      <td>0</td>
      <td>706</td>
      <td>1232</td>
      <td>557</td>
      <td>0</td>
      <td>407</td>
      <td>41</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>730</td>
      <td>0</td>
      <td>654</td>
      <td>717</td>
      <td>476</td>
      <td>0</td>
      <td>423</td>
      <td>72</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>652</td>
      <td>0</td>
      <td>759</td>
      <td>794</td>
      <td>569</td>
      <td>0</td>
      <td>709</td>
      <td>175</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>711</td>
      <td>0</td>
      <td>245</td>
      <td>2623</td>
      <td>64</td>
      <td>0</td>
      <td>19</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>0</td>
      <td>471</td>
      <td>0</td>
      <td>683</td>
      <td>553</td>
      <td>581</td>
      <td>0</td>
      <td>816</td>
      <td>327</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>0</td>
      <td>471</td>
      <td>0</td>
      <td>664</td>
      <td>396</td>
      <td>614</td>
      <td>0</td>
      <td>1071</td>
      <td>496</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>440</td>
      <td>0</td>
      <td>454</td>
      <td>244</td>
      <td>541</td>
      <td>0</td>
      <td>1244</td>
      <td>887</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>270</td>
      <td>0</td>
      <td>154</td>
      <td>121</td>
      <td>374</td>
      <td>0</td>
      <td>953</td>
      <td>1474</td>
    </tr>
  </tbody>
</table>
</div>



#### Random Forest with Categorical


```python
rf = RandomForestClassifier()
```


```python
rf.fit(X_train2,Y_train2)
```

    /anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
      "10 in version 0.20 to 100 in 0.22.", FutureWarning)





    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                max_depth=None, max_features='auto', max_leaf_nodes=None,
                min_impurity_decrease=0.0, min_impurity_split=None,
                min_samples_leaf=1, min_samples_split=2,
                min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,
                oob_score=False, random_state=None, verbose=0,
                warm_start=False)




```python
rf_pred_cat = rf.predict(X_test2)
```


```python
metrics.accuracy_score(Y_test2, rf_pred_cat)
```




    0.3947868735017772




```python
pd.DataFrame(metrics.confusion_matrix(Y_test2, rf_pred_cat))
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1626</td>
      <td>540</td>
      <td>411</td>
      <td>150</td>
      <td>154</td>
      <td>713</td>
      <td>72</td>
      <td>49</td>
      <td>33</td>
      <td>14</td>
    </tr>
    <tr>
      <th>1</th>
      <td>626</td>
      <td>1154</td>
      <td>568</td>
      <td>241</td>
      <td>268</td>
      <td>320</td>
      <td>143</td>
      <td>105</td>
      <td>65</td>
      <td>25</td>
    </tr>
    <tr>
      <th>2</th>
      <td>544</td>
      <td>532</td>
      <td>1537</td>
      <td>434</td>
      <td>405</td>
      <td>248</td>
      <td>247</td>
      <td>196</td>
      <td>126</td>
      <td>55</td>
    </tr>
    <tr>
      <th>3</th>
      <td>233</td>
      <td>323</td>
      <td>473</td>
      <td>867</td>
      <td>390</td>
      <td>105</td>
      <td>275</td>
      <td>200</td>
      <td>140</td>
      <td>66</td>
    </tr>
    <tr>
      <th>4</th>
      <td>213</td>
      <td>268</td>
      <td>450</td>
      <td>388</td>
      <td>1123</td>
      <td>102</td>
      <td>440</td>
      <td>325</td>
      <td>240</td>
      <td>109</td>
    </tr>
    <tr>
      <th>5</th>
      <td>691</td>
      <td>276</td>
      <td>201</td>
      <td>93</td>
      <td>72</td>
      <td>2237</td>
      <td>36</td>
      <td>32</td>
      <td>16</td>
      <td>9</td>
    </tr>
    <tr>
      <th>6</th>
      <td>145</td>
      <td>183</td>
      <td>280</td>
      <td>295</td>
      <td>482</td>
      <td>61</td>
      <td>1017</td>
      <td>484</td>
      <td>324</td>
      <td>160</td>
    </tr>
    <tr>
      <th>7</th>
      <td>93</td>
      <td>110</td>
      <td>233</td>
      <td>233</td>
      <td>361</td>
      <td>53</td>
      <td>471</td>
      <td>1281</td>
      <td>598</td>
      <td>279</td>
    </tr>
    <tr>
      <th>8</th>
      <td>59</td>
      <td>62</td>
      <td>144</td>
      <td>139</td>
      <td>230</td>
      <td>23</td>
      <td>357</td>
      <td>575</td>
      <td>1584</td>
      <td>637</td>
    </tr>
    <tr>
      <th>9</th>
      <td>28</td>
      <td>42</td>
      <td>74</td>
      <td>73</td>
      <td>109</td>
      <td>15</td>
      <td>169</td>
      <td>327</td>
      <td>607</td>
      <td>1902</td>
    </tr>
  </tbody>
</table>
</div>



##### K Nearest Neighbours


```python
K_value = list(range(1,15))
for k in K_value:
    neigh = KNeighborsClassifier(n_neighbors = k, weights='uniform', algorithm='auto')  
    neigh.fit(X_train2, Y_train2)  
    y_pred_cat = neigh.predict(X_test2)
    print("K =",k)
    print(metrics.accuracy_score(Y_test2,y_pred_cat))
    print(pd.DataFrame(metrics.confusion_matrix(Y_test2,y_pred_cat)))
    # Testing k nearest neighbors algorithm, looping through k=1 to k=25
    # Not Using error measures to evaluate model strength
    # Used confusion matrix
```

    K = 1
    0.45121648802799436
          0     1     2     3     4     5     6     7     8     9
    0  1765   496   351   168   143   545    96    89    68    41
    1   529  1332   501   229   279   248   142   125    89    41
    2   414   449  1741   460   377   224   233   210   141    75
    3   179   244   430  1134   377    97   213   201   130    67
    4   148   208   354   370  1480    96   411   284   206   101
    5   609   264   180   125   123  2116    91    65    50    40
    6   114   154   244   235   426    68  1316   476   276   122
    7    71   117   203   199   299    54   408  1569   559   233
    8    62    64   151   114   207    42   275   483  1883   529
    9    46    33    84    72    88    31   135   245   572  2040
    K = 2
    0.38602485327749153
          0     1     2    3     4     5     6     7     8     9
    0  2212   554   313  152   106   276    60    42    34    13
    1   903  1435   497  183   192   124    79    52    37    13
    2   750   751  1682  368   301   123   139   109    70    31
    3   371   421   609  986   309    62   134   102    55    23
    4   342   393   538  471  1271    72   288   147   101    35
    5  1027   419   286  187   157  1470    53    27    24    13
    6   237   283   421  339   549    88  1017   312   141    44
    7   180   247   342  299   432    83   539  1172   321    97
    8   133   135   265  208   319    68   379   635  1433   235
    9   108    72   153  134   174    31   219   359   764  1332
    K = 3
    0.3740666244179318
          0     1     2    3     4     5    6     7     8     9
    0  1928   526   343  160   119   479   71    51    56    29
    1   873  1285   490  175   189   203  106    96    69    29
    2   806   678  1496  354   303   183  164   142   116    82
    3   384   431   567  806   329   100  165   138    90    62
    4   406   416   496  378  1100   105  327   196   148    86
    5   916   332   239  139   111  1746   59    62    30    29
    6   300   320   394  321   413    85  892   368   218   120
    7   253   250   346  283   363    68  376  1159   405   209
    8   181   158   270  214   261    58  320   441  1445   462
    9   131    97   148  127   165    36  198   257   468  1719
    K = 4
    0.3601520954454027
          0     1     2    3     4     5    6     7     8     9
    0  1793   496   356  188   132   507   97    79    75    39
    1   770  1153   491  186   236   266  132   139    95    47
    2   734   592  1440  337   344   231  201   189   164    92
    3   386   368   546  765   320   137  189   167   117    77
    4   398   372   462  331  1081   148  337   243   181   105
    5   849   340   242  128   151  1715   80    79    43    36
    6   291   311   378  257   437   104  861   386   259   147
    7   244   239   306  273   321   100  394  1158   455   222
    8   182   152   269  159   260    81  281   478  1496   452
    9   128   107   131  111   145    55  157   251   652  1609
    K = 5
    0.35026038078968397
          0     1     2    3     4     5    6     7     8     9
    0  1682   489   386  173   155   530  110    94    87    56
    1   713  1090   527  199   249   264  153   148   111    61
    2   666   566  1420  354   350   262  228   203   174   101
    3   374   356   503  731   341   151  220   183   134    79
    4   382   345   466  339  1011   170  353   270   206   116
    5   774   340   244  156   147  1721   97    94    44    46
    6   283   289   391  267   444   126  820   391   254   166
    7   236   214   314  254   349   118  403  1116   459   249
    8   175   142   280  158   253    77  291   480  1461   493
    9   112   112   134  113   138    58  173   283   563  1660
    K = 6
    0.3368969222715124
          0     1     2    3    4     5    6     7     8     9
    0  1640   481   392  186  153   531  122   104    87    66
    1   728  1006   541  206  277   281  148   155   102    71
    2   699   531  1340  377  377   278  235   210   166   111
    3   369   350   513  692  352   161  211   192   136    96
    4   378   325   479  357  991   182  348   251   219   128
    5   784   339   288  153  169  1669   91    87    47    36
    6   281   287   392  270  457   147  808   386   256   147
    7   222   221   321  264  359   140  425  1066   439   255
    8   183   142   292  164  276    89  323   448  1414   479
    9   117   105   142  119  142    69  180   287   584  1601
    K = 7
    0.3289064006833274
          0    1     2    3    4     5    6     7     8     9
    0  1562  489   420  177  162   559  125   117    78    73
    1   718  984   538  215  270   296  163   147   107    77
    2   703  522  1323  370  365   271  249   223   171   127
    3   361  336   519  685  356   175  219   191   138    92
    4   393  344   482  355  930   185  361   251   223   134
    5   739  332   304  152  181  1687   77    90    59    42
    6   306  271   413  276  445   154  760   382   257   167
    7   254  224   323  285  362   140  385  1012   456   271
    8   181  154   291  189  276   100  307   424  1348   540
    9   129  100   147  117  150    68  180   257   552  1646
    K = 8
    0.3217424847766787
          0    1     2    3    4     5    6    7     8     9
    0  1550  469   438  171  166   565  121  115    88    79
    1   707  969   535  217  253   318  157  163   105    91
    2   714  518  1273  351  375   291  268  223   169   142
    3   389  310   523  632  349   199  227  187   158    98
    4   402  331   515  350  893   204  346  245   228   144
    5   761  327   319  139  165  1682   77   88    60    45
    6   304  283   420  262  444   168  727  369   265   189
    7   253  240   343  259  347   150  377  997   461   285
    8   193  158   306  179  280   100  268  435  1354   537
    9   134  104   150  124  166    68  156  257   587  1600
    K = 9
    0.3138070702339294
          0    1     2    3    4     5    6    7     8     9
    0  1470  466   446  190  192   579  120  120   100    79
    1   702  929   529  217  276   327  159  177   100    99
    2   718  506  1245  358  388   293  270  222   176   148
    3   362  315   538  576  365   206  240  201   164   105
    4   403  337   501  325  867   215  353  262   233   162
    5   745  304   320  142  160  1689   88   97    66    52
    6   300  288   414  243  422   187  713  386   274   204
    7   260  244   322  244  348   166  373  983   473   299
    8   217  169   296  186  264   106  260  411  1318   583
    9   142  113   170  114  164    75  150  263   556  1599
    K = 10
    0.3068911360317417
          0    1     2    3    4     5    6    7     8     9
    0  1448  468   430  188  172   609  134  117   110    86
    1   719  898   529  214  269   345  170  167    99   105
    2   679  500  1209  359  383   326  280  231   184   173
    3   377  334   520  548  360   211  237  210   161   114
    4   377  350   500  316  827   230  358  266   255   179
    5   760  311   316  133  156  1670   95  105    68    49
    6   294  296   414  233  416   208  688  389   290   203
    7   258  246   308  227  346   179  389  968   477   314
    8   233  162   306  184  248   105  265  400  1325   582
    9   153  114   157   99  154    84  165  250   613  1557
    K = 11
    0.30019563001129695
          0    1     2    3    4     5    6    7     8     9
    0  1424  453   439  182  183   610  138  120   109   104
    1   718  850   548  220  261   348  172  173   111   114
    2   688  488  1177  345  392   336  294  229   186   189
    3   386  314   507  538  362   216  248  212   167   122
    4   401  332   501  318  798   233  350  275   277   173
    5   747  324   321  133  159  1642  101  109    65    62
    6   295  291   413  234  419   211  680  377   303   208
    7   258  235   310  230  346   193  390  933   497   320
    8   225  167   324  173  271   115  263  377  1303   592
    9   161  110   167  100  171    93  163  247   584  1550
    K = 12
    0.29294905353649464
          0    1     2    3    4     5    6    7     8     9
    0  1378  445   455  179  192   629  135  131   106   112
    1   721  796   555  224  266   355  164  191   128   115
    2   684  476  1137  336  377   372  299  232   219   192
    3   390  320   519  510  359   223  243  207   175   126
    4   417  312   521  312  773   231  348  286   276   182
    5   755  313   307  141  166  1639   93  109    73    67
    6   276  289   423  219  424   219  668  398   306   209
    7   266  222   324  229  330   194  396  911   503   337
    8   222  171   312  174  266   128  262  386  1253   636
    9   172  106   176   94  183    90  153  244   561  1567
    K = 13
    0.2907172181963464
          0    1     2    3    4     5    6    7     8     9
    0  1371  453   447  158  179   641  146  136   117   114
    1   705  811   563  205  268   350  167  193   128   125
    2   689  472  1124  339  373   380  287  238   219   203
    3   389  316   530  488  345   234  244  213   189   124
    4   426  311   507  293  744   248  360  294   283   192
    5   723  309   307  144  175  1649   94  108    84    70
    6   289  281   417  210  412   230  655  395   310   232
    7   263  217   322  214  336   201  392  881   522   364
    8   211  171   328  155  250   143  259  401  1247   645
    9   169  107   169   96  187    99  151  228   559  1581
    K = 14
    0.2868321714931254
          0    1     2    3    4     5    6    7     8     9
    0  1340  443   453  162  168   663  160  140   108   125
    1   698  801   544  205  264   356  178  207   131   131
    2   694  467  1111  338  364   383  291  233   230   213
    3   409  314   526  471  332   224  237  225   194   140
    4   419  329   517  270  717   256  372  288   284   206
    5   725  298   311  138  186  1651   97   97    81    79
    6   283  277   414  201  423   238  654  388   314   239
    7   258  214   330  204  325   214  376  898   531   362
    8   218  171   329  151  253   153  253  405  1222   655
    9   176  104   179   91  192   103  151  236   569  1545



```python

```


```python

```
